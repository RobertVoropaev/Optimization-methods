{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Оптимальные методы первого порядка </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовили студенты 691 группы:\n",
    "#### Бармакова Азалия, Воропаев Роберт.\n",
    "#### Преподаватель: Тренин Сергей Алексеевич.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные определения\n",
    "Решение многих теоретических и практических задач сводится к отысканию экстремума скалярной функции $f(х)$ $n$-мерного векторного аргумента $x$. В дальнейшем под $x$ будем понимать вектор-столбец (точку в $n$-мерном пространстве). Оптимизируемую функцию $f(x)$ называют целевой функцией.\n",
    "\n",
    "В дальнейшем будем говорить о поиске минимального значения функции $f(x)$ записывать эту задачу следующим образом:\n",
    "$f(x) \\rightarrow min$.\n",
    "\n",
    "Вектор $х^*$, определяющий минимум целевой функции, называют оптимальным. Решением задачи (оптимальной точкой) называют допустимую точку $х^*$, в которой целевая функция $f(х)$ достигает своего минимального значения.\n",
    "\n",
    "В данном проекте мы рассматриваем задачу безусловной оптимизации выпуклой функции многих переменных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация методов\n",
    "Возможны два подхода к решению задачи отыскания минимума функции многих переменных $f(x) = f(x_1, ..., х_n)$ при отсутствии ограничений на диапазон изменения неизвестных. Первый подход лежит в основе косвенных методов оптимизации и сводит решение задачи оптимизации к решению системы нелинейных уравнений, являющихся следствием условий экстремума функции многих переменных. Как известно, эти условия определяют, что в точке экстремума $х^*$ все первые производные функции по независимым переменным равны нулю:\n",
    "\n",
    "$\\frac{df}{dx_i} = 0, i=1, …, n$.\n",
    "\n",
    "Эти условия образуют систему $n$ нелинейных уравнений, среди решений которой находятся точки минимума.\n",
    "\n",
    "Решение систем нелинейных уравнений - задача весьма сложная и трудоемкая. Вследствие этого на практике используют второй подход к минимизации функций, составляющий основу прямых методов. Суть их состоит в построении последовательности векторов \n",
    "$х_0, х_1, …, х_n,$ таких, что \n",
    "$f(х_0) > f(х_1) > f(х_n) >\\ldots $.\n",
    "В качестве начальной точки $х_0$ может быть выбрана произвольная точка, однако стремятся использовать всю имеющуюся информацию о поведении функции $f(x)$, чтобы точка $х_0$ располагалась как можно ближе к точке минимума. Переход (итерация) от точки $х_k$ к точке $х_{k+1}, k = 0, 1, 2, ...$, состоит из двух этапов:\n",
    "1.\tвыбор направления движения из точки $х_k$;\n",
    "2.\tопределение шага вдоль этого направления.\n",
    "\n",
    "Методы построения таких последовательностей часто называют методами спуска, так как осуществляется переход от больших значений функций к меньшим.\n",
    "Математически методы спуска описываются соотношением\n",
    "$x_{k+1} = x_k + a_k\\cdot p_k, k = 0, 1, 2, ...$,\n",
    "где $p_k$ - вектор, определяющий направление спуска; $a_k$ - длина шага.\n",
    " \n",
    "Различные методы спуска отличаются друг от друга способами выбора двух параметров - направления спуска и длины шага вдоль этого направления. На практике применяются только методы, обладающие сходимостью. Они позволяют за конечное число шагов получить точку минимума или подойти к точке, достаточно близкой к точке минимума. \n",
    "\n",
    "В методах спуска решение задачи теоретически получается за бесконечное число итераций. На практике вычисления прекращаются при выполнении некоторых критериев (условий) останова итерационного процесса. \n",
    "\n",
    "Методы поиска точки минимума называются детерминированными, если оба элемента перехода от $х_k$ к $x_{k+1}$ (направление движения и величина шага) выбираются однозначно по доступной в точке $х_k$ информации. Если же при переходе используется какой-либо случайный механизм, то алгоритм поиска называется случайным поиском минимума.\n",
    "\n",
    "Детерминированные алгоритмы безусловной минимизации делят на классы в зависимости от вида используемой информации. Если на каждой итерации используются лишь значения минимизируемых функций, то метод называется методом нулевого порядка. Если, кроме того, требуется вычисление первых производных минимизируемой функции, то имеют место методы первого порядка, при необходимости дополнительного вычисления вторых производных - методы второго порядка.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Минимизация функций многих переменных. \n",
    "Градиентом дифференцируемой функции $f(x)$ в точке $х_0$ называется $n$-мерный вектор $f(x_0)$, компоненты которого являются частными производными функции $f(х)$, вычисленными в точке $х_0$, т. е. \n",
    "\n",
    "$f'(x_0) = (\\frac{df(x_0)}{dх_1}, …, \\frac{df(x_0)}{dх_n})^T.$\n",
    "\n",
    "Этот вектор перпендикулярен к плоскости, проведенной через точку $х_0$, и касательной к поверхности уровня функции $f(x)$, проходящей через точку $х_0$.В каждой точке такой поверхности функция $f(x)$ принимает одинаковое значение. \n",
    "Вектор-градиент направлен в сторону наискорейшего возрастания функции в данной точке. Вектор, противоположный градиенту $(-f'(х_0))$, называется антиградиентом и направлен в сторону наискорейшего убывания функции. В точке минимума градиент функции равен нулю. На свойствах градиента основаны методы первого порядка, называемые также градиентным и методами минимизации. Использование этих методов в общем случае позволяет определить точку локального минимума функции.\n",
    "\n",
    "Очевидно, что если нет дополнительной информации, то из начальной точки $х_0$ разумно перейти в точку $х_1$, лежащую в направлении антиградиента - наискорейшего убывания функции. Выбирая в качестве направления спуска $p_k$ антиградиент $-f'(х_k)$ в точке $x_k$, получаем итерационный процесс вида \n",
    "\n",
    "$x_{k+1} = x_k + a_k\\cdot f'(x_k), a_k > 0,k = 0, 1, 2, ...$\n",
    "\n",
    "В качестве критерия останова итерационного процесса используют либо выполнение условия малости приращения аргумента \n",
    "\n",
    "$||x_k - x_{k+1}|| < \\varepsilon$, \n",
    "\n",
    "либо выполнение условия малости градиента \n",
    "\n",
    "$||f'(x_{k+1})|| \\leq \\gamma$. \n",
    "\n",
    "Здесь $k$ - номер итерации; $\\varepsilon, \\gamma$ - заданные величины точности решения задачи.Возможен и комбинированный критерий, состоящий в одновременном выполнении указанных условий. Градиентные методы отличаются друг от друга способами выбора величины шага $а_k$.\n",
    "\n",
    "При методе с постоянным шагом для всех итераций выбирается некоторая постоянная величина шага. Достаточно малый шаг $а_k$ обеспечит убывание функции, т. е. выполнение неравенства:\n",
    "\n",
    "$f(x_{k+1}) = f(x_k – akf’(x_k)) < f(x_k)$.\n",
    "\n",
    "Однако это может привести к необходимости проводить неприемлемо большое количество итераций для достижения точки минимума. С другой стороны, слишком большой шаг может вызвать неожиданный рост функции либо привести к колебаниям около точки минимума (зацикливанию). Из-за сложности получения необходимой информации для выбора величины шага методы с постоянным шагом применяются на практике редко.\n",
    "\n",
    "Более экономичны в смысле количества итераций и надежности градиентные методы с переменным шагом, когда в зависимости от результатов вычислений величина шага некоторым образом меняется из условия минимума функции $f(х)$ по $а$ в направлении движения, т. е. в результате решения задачи одномерной минимизации:\n",
    "$f(х_k + а_k\\cdot р_k) =  min(f(x_k + а\\cdot р_k)), a > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Минимизация функций многих переменных. \n",
    "Градиентом дифференцируемой функции $f(x)$ в точке $х_0$ называется $n$-мерный вектор $f(x_0)$, компоненты которого являются частными производными функции $f(х)$, вычисленными в точке $х_0$, т. е. \n",
    "\n",
    "$f'(x_0) = (\\frac{df(x_0)}{dх_1}, …, \\frac{df(x_0)}{dх_n})^T.$\n",
    "\n",
    "Этот вектор перпендикулярен к плоскости, проведенной через точку $х_0$, и касательной к поверхности уровня функции $f(x)$, проходящей через точку $х_0$.В каждой точке такой поверхности функция $f(x)$ принимает одинаковое значение. \n",
    "Вектор-градиент направлен в сторону наискорейшего возрастания функции в данной точке. Вектор, противоположный градиенту $(-f'(х_0))$, называется антиградиентом и направлен в сторону наискорейшего убывания функции. В точке минимума градиент функции равен нулю. На свойствах градиента основаны методы первого порядка, называемые также градиентным и методами минимизации. Использование этих методов в общем случае позволяет определить точку локального минимума функции.\n",
    "\n",
    "Очевидно, что если нет дополнительной информации, то из начальной точки $х_0$ разумно перейти в точку $х_1$, лежащую в направлении антиградиента - наискорейшего убывания функции. Выбирая в качестве направления спуска $p_k$ антиградиент $-f'(х_k)$ в точке $x_k$, получаем итерационный процесс вида \n",
    "\n",
    "$x_{k+1} = x_k + a_k\\cdot f'(x_k), a_k > 0,k = 0, 1, 2, ...$\n",
    "\n",
    "В качестве критерия останова итерационного процесса используют либо выполнение условия малости приращения аргумента \n",
    "\n",
    "$||x_k - x_{k+1}|| < \\varepsilon$, \n",
    "\n",
    "либо выполнение условия малости градиента \n",
    "\n",
    "$||f'(x_{k+1})|| \\leq \\gamma$. \n",
    "\n",
    "Здесь $k$ - номер итерации; $\\varepsilon, \\gamma$ - заданные величины точности решения задачи.Возможен и комбинированный критерий, состоящий в одновременном выполнении указанных условий. Градиентные методы отличаются друг от друга способами выбора величины шага $а_k$.\n",
    "\n",
    "При методе с постоянным шагом для всех итераций выбирается некоторая постоянная величина шага. Достаточно малый шаг $а_k$ обеспечит убывание функции, т. е. выполнение неравенства:\n",
    "\n",
    "$f(x_{k+1}) = f(x_k – akf’(x_k)) < f(x_k)$.\n",
    "\n",
    "Однако это может привести к необходимости проводить неприемлемо большое количество итераций для достижения точки минимума. С другой стороны, слишком большой шаг может вызвать неожиданный рост функции либо привести к колебаниям около точки минимума (зацикливанию). Из-за сложности получения необходимой информации для выбора величины шага методы с постоянным шагом применяются на практике редко.\n",
    "\n",
    "Более экономичны в смысле количества итераций и надежности градиентные методы с переменным шагом, когда в зависимости от результатов вычислений величина шага некоторым образом меняется из условия минимума функции $f(х)$ по $а$ в направлении движения, т. е. в результате решения задачи одномерной минимизации:\n",
    "$f(х_k + а_k\\cdot р_k) =  min(f(x_k + а\\cdot р_k)), a > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключаем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследуемая функция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\[  f(x, y) = \\frac{(x - 1)^2}{8} + \\frac{(x - 2)^2}{3}\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x[0] - 1)**2 / 8 + (x[1] - 2)**2 / 3\n",
    "    \n",
    "def df_1(x):\n",
    "    return (x[0] - 1) / 4\n",
    "\n",
    "def df_2(x):\n",
    "    return 2  / 3  * (x[1] - 2)\n",
    " \n",
    "def grad(x):\n",
    "    return np.array([df_1(x) , df_2(x)])\n",
    "\n",
    "x0, y0 = 454.95, -363.25\n",
    "eps = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод простого градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Алгоритм метода</h3>\n",
    "<ol>\n",
    "<li>Задаются начальное приближение и точность расчёта \n",
    "    $\\vec{x}^0, \\quad \\epsilon$ </li>\n",
    "<li>Рассчитывают\n",
    "$\\overrightarrow{x}^{[j+1]}=\\overrightarrow{x}^{[j]}-\\lambda\\nabla F(\\overrightarrow{x}^{[j]}) $, где $\\lambda=const$ </li>\n",
    "<li>Проверяют условие остановки:\n",
    "<ul>\n",
    "    <li>Если $|\\vec{x}^{[j+1]}-\\vec{x}^{[j]}|>\\epsilon$, то $j=j+1$ и переход к шагу 2.</li>\n",
    "\n",
    "<li>Иначе $\\vec{x}=\\vec{x}^{[j+1]}$ и останов. </li>\n",
    "</ul>\n",
    "</li>\n",
    "<ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meth1(x0, y0, print_flag):\n",
    "    n = 1000\n",
    "    x = np.zeros((n, 2))\n",
    "\n",
    "    lamb_const = 1/2\n",
    "    x[0] = x0, y0\n",
    "    i = 0\n",
    "    if print_flag:\n",
    "        print(i, x[i])\n",
    "\n",
    "    x[i + 1] = x[i] - lamb_const * grad(x[i])\n",
    "    if print_flag:\n",
    "        print(i + 1, x[i + 1])\n",
    "\n",
    "    while la.norm(x[i + 1] - x[i]) > eps:\n",
    "        i += 1\n",
    "        x[i + 1] = x[i] - lamb_const * grad(x[i])\n",
    "        if print_flag:\n",
    "            print(i + 1, x[i + 1])\n",
    "\n",
    "    i += 1\n",
    "    if print_flag:\n",
    "        print(\"Result:\", x[i], \"on step:\", i)\n",
    "    return x[i], i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем работу алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 454.95 -363.25]\n",
      "1 [ 398.20625 -241.5    ]\n",
      "2 [ 348.55546875 -160.33333333]\n",
      "3 [ 305.11103516 -106.22222222]\n",
      "4 [267.09715576 -70.14814815]\n",
      "5 [233.83501129 -46.09876543]\n",
      "6 [204.73063488 -30.06584362]\n",
      "7 [179.26430552 -19.37722908]\n",
      "8 [156.98126733 -12.25148605]\n",
      "9 [137.48360891  -7.5009907 ]\n",
      "10 [120.4231578  -4.3339938]\n",
      "11 [105.49526307  -2.22266253]\n",
      "12 [92.43335519 -0.81510836]\n",
      "13 [81.00418579  0.1232611 ]\n",
      "14 [71.00366257  0.74884073]\n",
      "15 [62.25320475  1.16589382]\n",
      "16 [54.59655415  1.44392921]\n",
      "17 [47.89698488  1.62928614]\n",
      "18 [42.03486177  1.75285743]\n",
      "19 [36.90550405  1.83523829]\n",
      "20 [32.41731605  1.89015886]\n",
      "21 [28.49015154  1.92677257]\n",
      "22 [25.0538826   1.95118171]\n",
      "23 [22.04714727  1.96745448]\n",
      "24 [19.41625386  1.97830298]\n",
      "25 [17.11422213  1.98553532]\n",
      "26 [15.09994436  1.99035688]\n",
      "27 [13.33745132  1.99357125]\n",
      "28 [11.7952699   1.99571417]\n",
      "29 [10.44586117  1.99714278]\n",
      "30 [9.26512852 1.99809519]\n",
      "31 [8.23198746 1.99873012]\n",
      "32 [7.32798902 1.99915342]\n",
      "33 [6.5369904  1.99943561]\n",
      "34 [5.8448666  1.99962374]\n",
      "35 [5.23925827 1.99974916]\n",
      "36 [4.70935099 1.99983277]\n",
      "37 [4.24568211 1.99988852]\n",
      "38 [3.83997185 1.99992568]\n",
      "39 [3.48497537 1.99995045]\n",
      "40 [3.17435345 1.99996697]\n",
      "41 [2.90255927 1.99997798]\n",
      "42 [2.66473936 1.99998532]\n",
      "43 [2.45664694 1.99999021]\n",
      "44 [2.27456607 1.99999348]\n",
      "45 [2.11524531 1.99999565]\n",
      "46 [1.97583965 1.9999971 ]\n",
      "47 [1.85385969 1.99999807]\n",
      "48 [1.74712723 1.99999871]\n",
      "49 [1.65373633 1.99999914]\n",
      "50 [1.57201929 1.99999943]\n",
      "51 [1.50051688 1.99999962]\n",
      "52 [1.43795227 1.99999975]\n",
      "53 [1.38320823 1.99999983]\n",
      "54 [1.3353072  1.99999989]\n",
      "55 [1.2933938  1.99999992]\n",
      "56 [1.25671958 1.99999995]\n",
      "57 [1.22462963 1.99999997]\n",
      "58 [1.19655093 1.99999998]\n",
      "59 [1.17198206 1.99999999]\n",
      "60 [1.1504843  1.99999999]\n",
      "61 [1.13167377 1.99999999]\n",
      "62 [1.11521454 2.        ]\n",
      "63 [1.10081273 2.        ]\n",
      "64 [1.08821114 2.        ]\n",
      "65 [1.07718474 2.        ]\n",
      "66 [1.06753665 2.        ]\n",
      "67 [1.05909457 2.        ]\n",
      "68 [1.05170775 2.        ]\n",
      "69 [1.04524428 2.        ]\n",
      "70 [1.03958874 2.        ]\n",
      "71 [1.03464015 2.        ]\n",
      "72 [1.03031013 2.        ]\n",
      "73 [1.02652137 2.        ]\n",
      "74 [1.0232062 2.       ]\n",
      "75 [1.02030542 2.        ]\n",
      "76 [1.01776724 2.        ]\n",
      "77 [1.01554634 2.        ]\n",
      "78 [1.01360305 2.        ]\n",
      "79 [1.01190266 2.        ]\n",
      "80 [1.01041483 2.        ]\n",
      "81 [1.00911298 2.        ]\n",
      "82 [1.00797386 2.        ]\n",
      "83 [1.00697712 2.        ]\n",
      "84 [1.00610498 2.        ]\n",
      "85 [1.00534186 2.        ]\n",
      "86 [1.00467413 2.        ]\n",
      "87 [1.00408986 2.        ]\n",
      "88 [1.00357863 2.        ]\n",
      "89 [1.0031313 2.       ]\n",
      "90 [1.00273989 2.        ]\n",
      "91 [1.0023974 2.       ]\n",
      "92 [1.00209773 2.        ]\n",
      "93 [1.00183551 2.        ]\n",
      "94 [1.00160607 2.        ]\n",
      "95 [1.00140531 2.        ]\n",
      "96 [1.00122965 2.        ]\n",
      "97 [1.00107594 2.        ]\n",
      "98 [1.00094145 2.        ]\n",
      "99 [1.00082377 2.        ]\n",
      "100 [1.0007208 2.       ]\n",
      "101 [1.0006307 2.       ]\n",
      "102 [1.00055186 2.        ]\n",
      "103 [1.00048288 2.        ]\n",
      "104 [1.00042252 2.        ]\n",
      "105 [1.0003697 2.       ]\n",
      "106 [1.00032349 2.        ]\n",
      "107 [1.00028305 2.        ]\n",
      "108 [1.00024767 2.        ]\n",
      "109 [1.00021671 2.        ]\n",
      "110 [1.00018962 2.        ]\n",
      "111 [1.00016592 2.        ]\n",
      "112 [1.00014518 2.        ]\n",
      "113 [1.00012703 2.        ]\n",
      "114 [1.00011115 2.        ]\n",
      "115 [1.00009726 2.        ]\n",
      "116 [1.0000851 2.       ]\n",
      "117 [1.00007446 2.        ]\n",
      "118 [1.00006516 2.        ]\n",
      "119 [1.00005701 2.        ]\n",
      "120 [1.00004989 2.        ]\n",
      "121 [1.00004365 2.        ]\n",
      "122 [1.00003819 2.        ]\n",
      "123 [1.00003342 2.        ]\n",
      "124 [1.00002924 2.        ]\n",
      "125 [1.00002559 2.        ]\n",
      "126 [1.00002239 2.        ]\n",
      "127 [1.00001959 2.        ]\n",
      "128 [1.00001714 2.        ]\n",
      "129 [1.000015 2.      ]\n",
      "130 [1.00001312 2.        ]\n",
      "131 [1.00001148 2.        ]\n",
      "132 [1.00001005 2.        ]\n",
      "133 [1.00000879 2.        ]\n",
      "134 [1.00000769 2.        ]\n",
      "135 [1.00000673 2.        ]\n",
      "136 [1.00000589 2.        ]\n",
      "137 [1.00000515 2.        ]\n",
      "138 [1.00000451 2.        ]\n",
      "139 [1.00000395 2.        ]\n",
      "140 [1.00000345 2.        ]\n",
      "141 [1.00000302 2.        ]\n",
      "142 [1.00000264 2.        ]\n",
      "143 [1.00000231 2.        ]\n",
      "144 [1.00000202 2.        ]\n",
      "145 [1.00000177 2.        ]\n",
      "146 [1.00000155 2.        ]\n",
      "147 [1.00000136 2.        ]\n",
      "148 [1.00000119 2.        ]\n",
      "149 [1.00000104 2.        ]\n",
      "150 [1.00000091 2.        ]\n",
      "151 [1.00000079 2.        ]\n",
      "152 [1.0000007 2.       ]\n",
      "153 [1.00000061 2.        ]\n",
      "154 [1.00000053 2.        ]\n",
      "155 [1.00000047 2.        ]\n",
      "156 [1.00000041 2.        ]\n",
      "157 [1.00000036 2.        ]\n",
      "158 [1.00000031 2.        ]\n",
      "159 [1.00000027 2.        ]\n",
      "160 [1.00000024 2.        ]\n",
      "161 [1.00000021 2.        ]\n",
      "162 [1.00000018 2.        ]\n",
      "163 [1.00000016 2.        ]\n",
      "164 [1.00000014 2.        ]\n",
      "165 [1.00000012 2.        ]\n",
      "166 [1.00000011 2.        ]\n",
      "167 [1.00000009 2.        ]\n",
      "168 [1.00000008 2.        ]\n",
      "169 [1.00000007 2.        ]\n",
      "170 [1.00000006 2.        ]\n",
      "171 [1.00000006 2.        ]\n",
      "172 [1.00000005 2.        ]\n",
      "173 [1.00000004 2.        ]\n",
      "174 [1.00000004 2.        ]\n",
      "175 [1.00000003 2.        ]\n",
      "176 [1.00000003 2.        ]\n",
      "177 [1.00000002 2.        ]\n",
      "178 [1.00000002 2.        ]\n",
      "179 [1.00000002 2.        ]\n",
      "180 [1.00000002 2.        ]\n",
      "181 [1.00000001 2.        ]\n",
      "182 [1.00000001 2.        ]\n",
      "183 [1.00000001 2.        ]\n",
      "184 [1.00000001 2.        ]\n",
      "185 [1.00000001 2.        ]\n",
      "186 [1.00000001 2.        ]\n",
      "187 [1.00000001 2.        ]\n",
      "188 [1.00000001 2.        ]\n",
      "189 [1. 2.]\n",
      "190 [1. 2.]\n",
      "191 [1. 2.]\n",
      "192 [1. 2.]\n",
      "193 [1. 2.]\n",
      "194 [1. 2.]\n",
      "195 [1. 2.]\n",
      "196 [1. 2.]\n",
      "197 [1. 2.]\n",
      "198 [1. 2.]\n",
      "199 [1. 2.]\n",
      "200 [1. 2.]\n",
      "201 [1. 2.]\n",
      "202 [1. 2.]\n",
      "203 [1. 2.]\n",
      "204 [1. 2.]\n",
      "Result: [1. 2.] on step: 204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 2.]), 204)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meth1(x0, y0, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод наискорейшего спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Алгоритм метода</h3>\n",
    "<ol>\n",
    "<li>Задаются начальное приближение и точность расчёта \n",
    "    $\\vec{x}^0, \\quad \\epsilon$ </li>\n",
    "<li>Рассчитывают\n",
    "$\\overrightarrow{x}^{[j+1]}=\\overrightarrow{x}^{[j]}-\\lambda^{[j]}\\nabla F(\\overrightarrow{x}^{[j]}) $, где $\\lambda^{[j]}=\\mathrm{argmin}_{\\lambda} \\,F(\\vec{x}^{[j]}-\\lambda^{[j]}\\nabla F(\\vec{x}^{[j]})) $ </li>\n",
    "<li>Проверяют условие остановки:\n",
    "<ul>\n",
    "    <li>Если $|\\vec{x}^{[j+1]}-\\vec{x}^{[j]}|>\\epsilon$, то $j=j+1$ и переход к шагу 2.</li>\n",
    "\n",
    "<li>Иначе $\\vec{x}=\\vec{x}^{[j+1]}$ и останов. </li>\n",
    "</ul>\n",
    "</li>\n",
    "<ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meth2(x0, y0, print_flag):\n",
    "    n = 1000\n",
    "    x = np.zeros((n, 2))\n",
    "\n",
    "    lamb = np.zeros(n)\n",
    "    def cal_lamb(x, i):\n",
    "        a = grad(x[i])[0]\n",
    "        b = grad(x[i])[1]\n",
    "        xx = x[i][0]\n",
    "        yy = x[i][1]\n",
    "        return (a**2 + b**2) / (0.25 * a**2 + b**2 *2 / 3)\n",
    "\n",
    "    x[0] = x0, y0\n",
    "    i = 0\n",
    "    if print_flag:\n",
    "        print(i, x[i])\n",
    "\n",
    "    lamb[i] = cal_lamb(x, i)\n",
    "    x[i + 1] = x[i] - lamb[i] * grad(x[i])\n",
    "    if print_flag:\n",
    "        print(i + 1, x[i + 1])\n",
    "\n",
    "    while la.norm(x[i + 1] - x[i]) > eps:\n",
    "        i += 1\n",
    "        lamb[i] = cal_lamb(x, i)\n",
    "        x[i + 1] = x[i] - lamb[i] * grad(x[i])\n",
    "        if print_flag:\n",
    "            print(i + 1, x[i + 1])\n",
    "\n",
    "    i += 1\n",
    "    if print_flag:\n",
    "        print(\"Result:\", x[i], \"on step:\", i)\n",
    "    return x[i], i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем работу алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 454.95 -363.25]\n",
      "1 [263.34857383  47.85207973]\n",
      "2 [ 61.14151986 -46.39010933]\n",
      "3 [35.75722428  8.07470815]\n",
      "4 [ 8.96784318 -4.41095874]\n",
      "5 [5.60480735 2.8048071 ]\n",
      "6 [2.0556189  1.15064478]\n",
      "7 [1.61006744 2.10662479]\n",
      "8 [1.13985356 1.88747326]\n",
      "9 [1.08082472 2.01412617]\n",
      "10 [1.01852849 1.98509191]\n",
      "11 [1.01070806 2.0018715 ]\n",
      "12 [1.00245474 1.9980249 ]\n",
      "13 [1.00141866 2.00024795]\n",
      "14 [1.00032522 1.99973833]\n",
      "15 [1.00018795 2.00003285]\n",
      "16 [1.00004309 1.99996533]\n",
      "17 [1.0000249  2.00000435]\n",
      "18 [1.00000571 1.99999541]\n",
      "19 [1.0000033  2.00000058]\n",
      "20 [1.00000076 1.99999939]\n",
      "21 [1.00000044 2.00000008]\n",
      "22 [1.0000001  1.99999992]\n",
      "23 [1.00000006 2.00000001]\n",
      "24 [1.00000001 1.99999999]\n",
      "25 [1.00000001 2.        ]\n",
      "26 [1. 2.]\n",
      "27 [1. 2.]\n",
      "28 [1. 2.]\n",
      "29 [1. 2.]\n",
      "30 [1. 2.]\n",
      "31 [1. 2.]\n",
      "Result: [1. 2.] on step: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 2.]), 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meth2(x0, y0, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод сопряженных градиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Алгоритм метода </h3>\n",
    "<ol>\n",
    "<li> Задаются начальным приближением и погрешностью: $\\vec{x}_0,\\quad \\varepsilon, \\quad k=0<$\n",
    "</li>\n",
    "<li> Рассчитывают начальное направление: $j=0,\\quad \\vec{S}_k^j=-\\nabla f(\\vec{x}_k),\\quad \\vec{x}_k^j=\\vec{x}_k$\n",
    "</li>\n",
    "<li>Рассчитывают $\\vec{x}_k^{j+1}=\\vec{x}_k^j+\\lambda\\vec{S}_k^j,\\quad \\lambda=\\arg\\min_\\lambda f(\\vec{x}_k^j+\\lambda \\vec{S}_k^j),\\quad \\vec{S}_k^{j+1}=-\\nabla f(\\vec{x}_k^{j+1})+\\omega \\vec{S}_k^j, \\omega=\\frac{||\\nabla f(\\vec{x}_k^{j+1})||^2}{||\\nabla f(\\vec{x}_k^{j})||^2}$\n",
    "<ul> \n",
    "<li>Если $||\\vec{S}_k^{j+1}||<\\varepsilon$ или $||\\vec{x}_k^{j+1}-\\vec{x}_k^j||<\\varepsilon$, то $\\vec{x}=\\vec{x}_k^{j+1}$ и останов.\n",
    "    </li>\n",
    "<li>Иначе\n",
    " если $(j+1)<n$, то $j=j+1$ и переход к 3;\n",
    " иначе $\\vec{x}_{k+1}=\\vec{x}_k^{j+1},\\quad k=k+1$ и переход к 2.\n",
    "    </li>\n",
    "    </ul>\n",
    "</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meth3(x0, y0, print_flag):\n",
    "    n = 1000\n",
    "    x = np.zeros((n, 2))\n",
    "\n",
    "\n",
    "    lamb = np.zeros(n)\n",
    "    def cal_lamb(x, i):\n",
    "        a = grad(x[i])[0]\n",
    "        b = grad(x[i])[1]\n",
    "        xx = x[i][0]\n",
    "        yy = x[i][1]\n",
    "        return (a**2 + b**2) / (0.25 * a**2 + b**2 *2 / 3)\n",
    "\n",
    "    x[0] = x0, y0\n",
    "    i = 0\n",
    "    k = 0\n",
    "    if print_flag:\n",
    "        print(i, x[i])\n",
    "        \n",
    "    s = grad(x[k])\n",
    "    lamb[i] = cal_lamb(x, i)\n",
    "    x[i + 1] = x[i] + lamb[i] * s[i]\n",
    "    omega = la.norm(grad(x[i + 1])) ** 2 / la.norm(grad(x[i])) **2\n",
    "    s *= omega\n",
    "    s += -grad(x[i + 1])\n",
    "    if (print_flag):\n",
    "        print(i + 1, x[i + 1])\n",
    "\n",
    "    while la.norm(x[i + 1] - x[i]) > eps:\n",
    "        i += 1\n",
    "        lamb[i] = cal_lamb(x, i)\n",
    "        x[i + 1] = x[i] + lamb[i] * s\n",
    "        omega = la.norm(grad(x[i + 1])) ** 2 / la.norm(grad(x[i])) **2\n",
    "        s *= omega\n",
    "        s += -grad(x[i + 1])\n",
    "        if (print_flag):\n",
    "            print(i + 1, x[i + 1])\n",
    "\n",
    "    i += 1\n",
    "    if (print_flag):\n",
    "        print(\"Result:\", x[i], \"on step:\", i)\n",
    "    return x[i], i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем работу алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 454.95 -363.25]\n",
      "1 [ 646.55142617 -171.64857383]\n",
      "2 [ 392.80033855 -215.90037501]\n",
      "3 [66.07790486 29.7639999 ]\n",
      "4 [25.38810183 -3.02013781]\n",
      "5 [3.25328776 2.98330676]\n",
      "6 [1.86102671 1.71019431]\n",
      "7 [1.1920549  1.99325691]\n",
      "8 [0.97078237 2.02450482]\n",
      "9 [0.97007326 1.99897897]\n",
      "10 [0.99930153 1.99106269]\n",
      "11 [1.00656193 1.99810934]\n",
      "12 [1.00394804 2.00262171]\n",
      "13 [1.00044442 2.00257076]\n",
      "14 [0.99809428 1.99996151]\n",
      "15 [0.99951076 1.99953062]\n",
      "16 [0.99999935 1.99995461]\n",
      "17 [1.00000323 2.00000316]\n",
      "18 [1.00000194 2.00000002]\n",
      "19 [0.99999986 1.99999962]\n",
      "20 [0.99999969 1.99999996]\n",
      "21 [0.99999992 2.00000013]\n",
      "22 [1.00000007 2.00000008]\n",
      "23 [1.00000011 1.99999997]\n",
      "24 [1.00000006 1.99999996]\n",
      "25 [1. 2.]\n",
      "26 [1. 2.]\n",
      "27 [1. 2.]\n",
      "28 [1. 2.]\n",
      "29 [1. 2.]\n",
      "Result: [1. 2.] on step: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 2.]), 29)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meth3(x0, y0, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как алгоритмы могут давать существенно разные результаты для различных точек, то будет суммировать из результаты, полученные на большом числе итераций, для каждой из которых будем генерировать случайную точку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика за 5000 итераций для достижения точности в 1e-10\n",
      "1.Метод наискорейшего спуска\n",
      "2.Метод сопряженных градиентов\n",
      "3.Метод простого градиентного спуска\n",
      "\n",
      "Суммарное число шагов\n",
      "755245.0 64241.0 135467.0\n",
      "\n",
      "Среднее число шагов\n",
      "151.049 12.8482 27.0934\n",
      "\n",
      "Максимальное число шагов\n",
      "158.0 22.0 47.0\n",
      "\n",
      "Минимальное число шагов\n",
      "96.0 4.0 18.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5000\n",
    "s = np.zeros((3, n))\n",
    "\n",
    "for i in range(0, n):\n",
    "    x, y = r.random(), r.random()\n",
    "    s[0][i] = meth1(x, y, False)[1]\n",
    "    s[1][i] = meth2(x, y, False)[1]\n",
    "    s[2][i] = meth3(x, y, False)[1]\n",
    "\n",
    "print(\"Статистика за {} итераций для достижения точности в 1e-10\".format(n))\n",
    "print(\"1.Метод наискорейшего спуска\")\n",
    "print(\"2.Метод сопряженных градиентов\")\n",
    "print(\"3.Метод простого градиентного спуска\")\n",
    "print()\n",
    "\n",
    "print(\"Суммарное число шагов\")\n",
    "print(np.sum(s[0]), np.sum(s[1]), np.sum(s[2]))\n",
    "print()\n",
    "\n",
    "print(\"Среднее число шагов\")\n",
    "print(np.average(s[0]), np.average(s[1]), np.average(s[2]))\n",
    "print()\n",
    "\n",
    "print(\"Максимальное число шагов\")\n",
    "print(np.max(s[0]), np.max(s[1]), np.max(s[2]))\n",
    "print()\n",
    "\n",
    "print(\"Минимальное число шагов\")\n",
    "print(np.min(s[0]), np.min(s[1]), np.min(s[2]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод практической части"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По статистике, собранной нами при сравнении алгоритмов, можно сказать, что метод наискорейшего спуска является самым оптимальным для нашей функции.\n",
    "\n",
    "При различных оптимизируемых функциях и начальных точках, результаты (среднее, максимальное и минимальное число шагов) могут сильно отличаться.\n",
    "\n",
    "Полный список методов от лучшего к худшему по эффективности:\n",
    "<ol>\n",
    "<li>Метод наискорейшего спуска\n",
    "<li>Метод сопряженных градиентов\n",
    "<li>Метод простого градиентного спуска\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
